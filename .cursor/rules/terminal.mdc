---
description: Terminal optimization guide for Claude agents - deterministic, efficient, and composable command patterns
alwaysApply: false
---

# Claude Agent Terminal Command Patterns

## Core Principles for AI Agents

### 1. VALIDATE BEFORE EXECUTE
```bash
# Every command should validate preconditions
validate_then_execute() {
    local cmd="$1"
    local validators="$2"
    
    # Run validators first
    for validator in $validators; do
        $validator || {
            echo '{"error": "validation_failed", "validator": "'$validator'", "command": "'$cmd'"}'
            return 1
        }
    done
    
    # Execute with structured output
    local start_time=$(date +%s.%N)
    local output=$(eval "$cmd" 2>&1)
    local exit_code=$?
    local end_time=$(date +%s.%N)
    
    # Return structured result
    jq -n \
        --arg cmd "$cmd" \
        --arg output "$output" \
        --arg exit_code "$exit_code" \
        --arg duration "$(echo "$end_time - $start_time" | bc)" \
        '{command: $cmd, output: $output, exit_code: $exit_code | tonumber, duration: $duration | tonumber}'
}
```

### 2. STRUCTURED OUTPUT ALWAYS
```bash
# All commands should return parseable output
export AGENT_OUTPUT_FORMAT="json"

# Wrap any command to ensure structured output
structured_exec() {
    local cmd="$@"
    local temp_file=$(mktemp)
    local error_file=$(mktemp)
    
    # Execute with full capture
    {
        eval "$cmd"
    } > "$temp_file" 2> "$error_file"
    local exit_code=$?
    
    # Build structured response
    jq -n \
        --arg stdout "$(cat "$temp_file")" \
        --arg stderr "$(cat "$error_file")" \
        --arg exit_code "$exit_code" \
        --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        '{
            stdout: $stdout,
            stderr: $stderr,
            exit_code: $exit_code | tonumber,
            timestamp: $timestamp,
            success: ($exit_code == "0")
        }'
    
    rm -f "$temp_file" "$error_file"
    return $exit_code
}
```

### 3. IDEMPOTENT OPERATIONS
```bash
# Make operations safe to retry
idempotent_operation() {
    local operation_id="$1"
    local operation_cmd="$2"
    local state_dir="${STATE_DIR:-/tmp/agent_state}"
    local state_file="$state_dir/op_${operation_id}.json"
    
    mkdir -p "$state_dir"
    
    # Check if already completed
    if [ -f "$state_file" ]; then
        local prev_result=$(cat "$state_file")
        local prev_exit=$(echo "$prev_result" | jq -r '.exit_code')
        [ "$prev_exit" = "0" ] && {
            echo "$prev_result" | jq '. + {cached: true}'
            return 0
        }
    fi
    
    # Execute and save state
    local result=$(structured_exec "$operation_cmd")
    echo "$result" > "$state_file"
    echo "$result"
}
```

## File Operations for Agents

### Safe File Reading
```bash
# Read files with validation and limits
safe_read_file() {
    local file="$1"
    local max_size="${2:-10485760}"  # 10MB default
    
    # Validation checks
    [ -f "$file" ] || {
        echo '{"error": "file_not_found", "path": "'$file'"}'
        return 1
    }
    
    [ -r "$file" ] || {
        echo '{"error": "permission_denied", "path": "'$file'"}'
        return 1
    }
    
    local size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
    [ "$size" -gt "$max_size" ] && {
        echo '{"error": "file_too_large", "size": '$size', "max_size": '$max_size'}'
        return 1
    }
    
    # Read with metadata
    jq -n \
        --arg content "$(cat "$file" | head -c "$max_size")" \
        --arg path "$file" \
        --arg size "$size" \
        --arg mime "$(file -b --mime-type "$file")" \
        --arg encoding "$(file -b --mime-encoding "$file")" \
        '{
            path: $path,
            content: $content,
            size: $size | tonumber,
            mime_type: $mime,
            encoding: $encoding,
            truncated: ($size > ($content | length))
        }'
}

# Batch file operations
batch_file_operation() {
    local operation="$1"
    local pattern="$2"
    local max_files="${3:-100}"
    
    # Find files safely
    local files=$(find . -type f -name "$pattern" 2>/dev/null | head -n "$max_files")
    local file_count=$(echo "$files" | wc -l)
    
    [ -z "$files" ] && {
        echo '{"error": "no_files_found", "pattern": "'$pattern'"}'
        return 1
    }
    
    # Process in parallel with structured results
    echo "$files" | parallel -j4 --keep-order "
        result=\$(structured_exec '$operation {}')
        jq -n \
            --arg file '{}' \
            --argjson result \"\$result\" \
            '{file: \$file, result: \$result}'
    " | jq -s '{
        total_files: '$file_count',
        processed: length,
        results: .
    }'
}
```

### Safe File Writing
```bash
# Write files with atomic operations and validation
safe_write_file() {
    local file="$1"
    local content="$2"
    local backup="${3:-true}"
    
    # Validate path
    local dir=$(dirname "$file")
    [ -d "$dir" ] || mkdir -p "$dir" || {
        echo '{"error": "cannot_create_directory", "path": "'$dir'"}'
        return 1
    }
    
    # Check permissions
    [ -w "$dir" ] || {
        echo '{"error": "directory_not_writable", "path": "'$dir'"}'
        return 1
    }
    
    # Backup if exists
    if [ -f "$file" ] && [ "$backup" = "true" ]; then
        local backup_file="${file}.backup.$(date +%s)"
        cp "$file" "$backup_file" || {
            echo '{"error": "backup_failed", "path": "'$file'"}'
            return 1
        }
    fi
    
    # Atomic write
    local temp_file=$(mktemp)
    echo "$content" > "$temp_file" || {
        echo '{"error": "write_failed", "path": "'$temp_file'"}'
        rm -f "$temp_file"
        return 1
    }
    
    # Move atomically
    mv -f "$temp_file" "$file" || {
        echo '{"error": "move_failed", "source": "'$temp_file'", "dest": "'$file'"}'
        rm -f "$temp_file"
        return 1
    }
    
    # Return success with metadata
    jq -n \
        --arg path "$file" \
        --arg size "$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file")" \
        --arg backup "${backup_file:-none}" \
        '{
            success: true,
            path: $path,
            size: $size | tonumber,
            backup: $backup,
            timestamp: now | todate
        }'
}
```

## Process Management for Agents

### Managed Process Execution
```bash
# Execute processes with timeouts and resource limits
managed_process() {
    local cmd="$1"
    local timeout="${2:-30}"
    local max_mem="${3:-512M}"
    local work_dir="${4:-.}"
    
    # Validate working directory
    [ -d "$work_dir" ] || {
        echo '{"error": "invalid_working_directory", "path": "'$work_dir'"}'
        return 1
    }
    
    # Create process wrapper
    local process_id="proc_$(date +%s)_$$"
    local status_file="/tmp/${process_id}.status"
    
    # Execute with limits
    (
        cd "$work_dir"
        ulimit -v $(echo "$max_mem" | sed 's/[^0-9]//g')
        timeout "$timeout" bash -c "$cmd"
        echo $? > "$status_file"
    ) 2>&1 | {
        local output=""
        local line_count=0
        local max_lines=1000
        
        while IFS= read -r line && [ $line_count -lt $max_lines ]; do
            output="${output}${line}\n"
            ((line_count++))
        done
        
        # Get exit status
        local exit_code=$(cat "$status_file" 2>/dev/null || echo "124")
        rm -f "$status_file"
        
        # Return structured result
        jq -n \
            --arg cmd "$cmd" \
            --arg output "$output" \
            --arg exit_code "$exit_code" \
            --arg timeout "$timeout" \
            --arg work_dir "$work_dir" \
            --arg lines "$line_count" \
            '{
                command: $cmd,
                output: $output,
                exit_code: $exit_code | tonumber,
                timeout_seconds: $timeout | tonumber,
                working_directory: $work_dir,
                output_lines: $lines | tonumber,
                truncated: ($lines >= 1000),
                timed_out: ($exit_code == "124")
            }'
    }
}

# Background task management
background_task() {
    local task_name="$1"
    local cmd="$2"
    local task_file="/tmp/agent_task_${task_name}.json"
    
    # Check if already running
    if [ -f "$task_file" ]; then
        local pid=$(jq -r '.pid' "$task_file")
        if kill -0 "$pid" 2>/dev/null; then
            echo '{"error": "task_already_running", "task": "'$task_name'", "pid": '$pid'}'
            return 1
        fi
    fi
    
    # Start task
    {
        eval "$cmd"
    } > "/tmp/agent_task_${task_name}.out" 2>&1 &
    
    local pid=$!
    
    # Save task info
    jq -n \
        --arg name "$task_name" \
        --arg cmd "$cmd" \
        --arg pid "$pid" \
        --arg start "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        '{
            task_name: $name,
            command: $cmd,
            pid: $pid | tonumber,
            started_at: $start,
            status: "running"
        }' > "$task_file"
    
    echo "$task_file" | jq -R -s '{success: true, task_file: .}'
}
```

## Git Operations for Agents

### Safe Git Commands
```bash
# Check git status with structured output
git_status_structured() {
    local repo_path="${1:-.}"
    
    cd "$repo_path" 2>/dev/null || {
        echo '{"error": "not_a_directory", "path": "'$repo_path'"}'
        return 1
    }
    
    git rev-parse --git-dir >/dev/null 2>&1 || {
        echo '{"error": "not_a_git_repository", "path": "'$repo_path'"}'
        return 1
    }
    
    # Gather comprehensive status
    local branch=$(git branch --show-current)
    local ahead_behind=$(git rev-list --left-right --count @{u}...HEAD 2>/dev/null || echo "0 0")
    local ahead=$(echo "$ahead_behind" | cut -f2)
    local behind=$(echo "$ahead_behind" | cut -f1)
    
    # Get file states
    local modified=$(git ls-files -m | wc -l)
    local untracked=$(git ls-files -o --exclude-standard | wc -l)
    local staged=$(git diff --cached --name-only | wc -l)
    
    jq -n \
        --arg branch "$branch" \
        --arg ahead "$ahead" \
        --arg behind "$behind" \
        --arg modified "$modified" \
        --arg untracked "$untracked" \
        --arg staged "$staged" \
        --arg clean "$([ $modified -eq 0 ] && [ $untracked -eq 0 ] && [ $staged -eq 0 ] && echo true || echo false)" \
        '{
            branch: $branch,
            ahead: $ahead | tonumber,
            behind: $behind | tonumber,
            modified_files: $modified | tonumber,
            untracked_files: $untracked | tonumber,
            staged_files: $staged | tonumber,
            clean: $clean | test("true")
        }'
}

# Safe git operations with validation
git_safe_operation() {
    local operation="$1"
    shift
    local args="$@"
    
    # Validate repository state first
    local status=$(git_status_structured)
    echo "$status" | jq -e '.error' >/dev/null && {
        echo "$status"
        return 1
    }
    
    # Operation-specific validation
    case "$operation" in
        commit)
            # Check if there are staged changes
            local staged=$(echo "$status" | jq -r '.staged_files')
            [ "$staged" -eq 0 ] && {
                echo '{"error": "nothing_to_commit", "hint": "Stage changes first with git add"}'
                return 1
            }
            ;;
        push)
            # Check if we have a remote
            git remote -v | grep -q push || {
                echo '{"error": "no_remote_configured"}'
                return 1
            }
            ;;
        pull)
            # Check for uncommitted changes
            local modified=$(echo "$status" | jq -r '.modified_files')
            [ "$modified" -gt 0 ] && {
                echo '{"error": "uncommitted_changes", "hint": "Commit or stash changes first"}'
                return 1
            }
            ;;
    esac
    
    # Execute operation
    structured_exec "git $operation $args"
}
```

## Package Management for Agents

### Universal Package Manager Interface
```bash
# Detect and use appropriate package manager
detect_package_manager() {
    local project_path="${1:-.}"
    
    cd "$project_path" 2>/dev/null || return 1
    
    # Check for lock files in order of preference
    [ -f "bun.lockb" ] && echo "bun" && return 0
    [ -f "pnpm-lock.yaml" ] && echo "pnpm" && return 0
    [ -f "yarn.lock" ] && echo "yarn" && return 0
    [ -f "package-lock.json" ] && echo "npm" && return 0
    [ -f "Cargo.lock" ] && echo "cargo" && return 0
    [ -f "go.sum" ] && echo "go" && return 0
    [ -f "Pipfile.lock" ] && echo "pipenv" && return 0
    [ -f "poetry.lock" ] && echo "poetry" && return 0
    [ -f "requirements.txt" ] && echo "pip" && return 0
    
    echo "none"
}

# Universal install command
universal_install() {
    local project_path="${1:-.}"
    local package_manager=$(detect_package_manager "$project_path")
    
    [ "$package_manager" = "none" ] && {
        echo '{"error": "no_package_manager_detected", "path": "'$project_path'"}'
        return 1
    }
    
    # Define install commands
    case "$package_manager" in
        bun) cmd="bun install" ;;
        pnpm) cmd="pnpm install --frozen-lockfile" ;;
        yarn) cmd="yarn install --frozen-lockfile" ;;
        npm) cmd="npm ci" ;;
        cargo) cmd="cargo build --release" ;;
        go) cmd="go mod download && go build" ;;
        pipenv) cmd="pipenv install --deploy" ;;
        poetry) cmd="poetry install --no-dev" ;;
        pip) cmd="pip install -r requirements.txt" ;;
    esac
    
    # Execute with monitoring
    jq -n \
        --arg pm "$package_manager" \
        --arg cmd "$cmd" \
        --arg path "$project_path" \
        '{
            package_manager: $pm,
            command: $cmd,
            project_path: $path,
            started_at: now | todate
        }'
    
    managed_process "$cmd" 300 "2G" "$project_path"
}
```

## Resource Management

### System Resource Checks
```bash
# Check system resources before operations
check_resources() {
    local required_disk="${1:-1073741824}"  # 1GB default
    local required_mem="${2:-536870912}"     # 512MB default
    local required_cpu="${3:-20}"            # 20% idle
    
    # Disk space check
    local available_disk=$(df -B1 . | awk 'NR==2 {print $4}')
    [ "$available_disk" -lt "$required_disk" ] && {
        jq -n \
            --arg have "$available_disk" \
            --arg need "$required_disk" \
            '{
                error: "insufficient_disk_space",
                available_bytes: $have | tonumber,
                required_bytes: $need | tonumber
            }'
        return 1
    }
    
    # Memory check
    local available_mem=$(free -b | awk 'NR==2 {print $7}')
    [ "$available_mem" -lt "$required_mem" ] && {
        jq -n \
            --arg have "$available_mem" \
            --arg need "$required_mem" \
            '{
                error: "insufficient_memory",
                available_bytes: $have | tonumber,
                required_bytes: $need | tonumber
            }'
        return 1
    }
    
    # CPU check
    local cpu_idle=$(top -bn1 | grep "Cpu(s)" | awk '{print $8}' | cut -d'%' -f1)
    [ "${cpu_idle%.*}" -lt "$required_cpu" ] && {
        jq -n \
            --arg have "$cpu_idle" \
            --arg need "$required_cpu" \
            '{
                error: "insufficient_cpu",
                idle_percent: $have | tonumber,
                required_percent: $need | tonumber
            }'
        return 1
    }
    
    # All checks passed
    jq -n \
        --arg disk "$available_disk" \
        --arg mem "$available_mem" \
        --arg cpu "$cpu_idle" \
        '{
            resources_ok: true,
            disk_bytes: $disk | tonumber,
            memory_bytes: $mem | tonumber,
            cpu_idle_percent: $cpu | tonumber
        }'
}

# Resource-aware command execution
resource_aware_exec() {
    local cmd="$1"
    local disk_needed="${2:-1073741824}"
    local mem_needed="${3:-536870912}"
    
    # Check resources first
    local resource_check=$(check_resources "$disk_needed" "$mem_needed")
    echo "$resource_check" | jq -e '.error' >/dev/null && {
        echo "$resource_check"
        return 1
    }
    
    # Execute with resource monitoring
    managed_process "$cmd"
}
```

## Caching for Agents

### Deterministic Cache Keys
```bash
# Generate stable cache keys
generate_cache_key() {
    local namespace="$1"
    local input="$2"
    local context="$3"
    
    # Create deterministic key
    echo "${namespace}:${input}:${context}" | sha256sum | cut -d' ' -f1
}

# Time-based cache with TTL
timed_cache() {
    local key="$1"
    local ttl="${2:-3600}"
    local generator="$3"
    local cache_dir="${AGENT_CACHE_DIR:-/tmp/agent_cache}"
    
    mkdir -p "$cache_dir"
    local cache_file="$cache_dir/${key}.cache"
    local meta_file="$cache_dir/${key}.meta"
    
    # Check cache validity
    if [ -f "$cache_file" ] && [ -f "$meta_file" ]; then
        local cached_at=$(jq -r '.cached_at' "$meta_file")
        local now=$(date +%s)
        local age=$((now - cached_at))
        
        if [ "$age" -lt "$ttl" ]; then
            # Cache hit
            jq -n \
                --arg data "$(cat "$cache_file")" \
                --arg age "$age" \
                --arg ttl "$ttl" \
                '{
                    cache_hit: true,
                    data: $data,
                    age_seconds: $age | tonumber,
                    ttl_seconds: $ttl | tonumber
                }'
            return 0
        fi
    fi
    
    # Cache miss - generate new data
    local data=$(eval "$generator")
    local exit_code=$?
    
    if [ "$exit_code" -eq 0 ]; then
        # Save to cache
        echo "$data" > "$cache_file"
        jq -n --arg time "$(date +%s)" '{cached_at: $time | tonumber}' > "$meta_file"
        
        jq -n \
            --arg data "$data" \
            '{
                cache_hit: false,
                data: $data,
                cached: true
            }'
    else
        # Don't cache failures
        jq -n \
            --arg data "$data" \
            '{
                cache_hit: false,
                data: $data,
                cached: false,
                error: true
            }'
    fi
    
    return $exit_code
}
```

## Error Handling Patterns

### Structured Error Recovery
```bash
# Retry with exponential backoff
retry_with_backoff() {
    local cmd="$1"
    local max_attempts="${2:-3}"
    local base_delay="${3:-1}"
    
    local attempt=0
    local delay=$base_delay
    
    while [ "$attempt" -lt "$max_attempts" ]; do
        # Try command
        local result=$(structured_exec "$cmd")
        local exit_code=$(echo "$result" | jq -r '.exit_code')
        
        if [ "$exit_code" -eq 0 ]; then
            # Success
            echo "$result" | jq '. + {attempts: '$((attempt + 1))'}'
            return 0
        fi
        
        # Failed - wait and retry
        ((attempt++))
        if [ "$attempt" -lt "$max_attempts" ]; then
            jq -n \
                --arg attempt "$attempt" \
                --arg delay "$delay" \
                '{
                    retry_attempt: $attempt | tonumber,
                    waiting_seconds: $delay | tonumber,
                    status: "retrying"
                }' >&2
            sleep "$delay"
            delay=$((delay * 2))
        fi
    done
    
    # All attempts failed
    echo "$result" | jq '. + {
        attempts: '$max_attempts',
        failed: true,
        error: "max_retries_exceeded"
    }'
    return 1
}

# Circuit breaker pattern
circuit_breaker() {
    local service="$1"
    local cmd="$2"
    local failure_threshold="${3:-5}"
    local reset_timeout="${4:-60}"
    
    local state_file="/tmp/circuit_${service}.json"
    
    # Check circuit state
    if [ -f "$state_file" ]; then
        local state=$(jq -r '.state' "$state_file")
        local last_failure=$(jq -r '.last_failure' "$state_file")
        local failures=$(jq -r '.failures' "$state_file")
        local now=$(date +%s)
        
        if [ "$state" = "open" ]; then
            # Check if we should try to close
            local time_since_failure=$((now - last_failure))
            if [ "$time_since_failure" -lt "$reset_timeout" ]; then
                echo '{"error": "circuit_open", "service": "'$service'", "retry_after": '$((reset_timeout - time_since_failure))'}'
                return 1
            fi
            # Half-open state - try once
            state="half-open"
        fi
    else
        # Initialize state
        jq -n '{state: "closed", failures: 0, last_failure: 0}' > "$state_file"
    fi
    
    # Execute command
    local result=$(structured_exec "$cmd")
    local exit_code=$(echo "$result" | jq -r '.exit_code')
    
    if [ "$exit_code" -eq 0 ]; then
        # Success - reset circuit
        jq -n '{state: "closed", failures: 0, last_failure: 0}' > "$state_file"
        echo "$result"
        return 0
    else
        # Failure - update circuit
        local new_failures=$((failures + 1))
        if [ "$new_failures" -ge "$failure_threshold" ]; then
            # Open circuit
            jq -n \
                --arg now "$(date +%s)" \
                --arg failures "$new_failures" \
                '{
                    state: "open",
                    failures: $failures | tonumber,
                    last_failure: $now | tonumber
                }' > "$state_file"
        else
            # Increment failure count
            jq ".failures = $new_failures" "$state_file" > "${state_file}.tmp" && mv "${state_file}.tmp" "$state_file"
        fi
        echo "$result"
        return 1
    fi
}
```

## Agent-Specific Helpers

### Command Batching
```bash
# Batch multiple commands efficiently
batch_commands() {
    local commands="$1"  # JSON array of commands
    local parallel="${2:-false}"
    local max_parallel="${3:-4}"
    
    # Validate input
    echo "$commands" | jq -e '.' >/dev/null 2>&1 || {
        echo '{"error": "invalid_json_input"}'
        return 1
    }
    
    if [ "$parallel" = "true" ]; then
        # Parallel execution
        echo "$commands" | jq -r '.[]' | parallel -j "$max_parallel" --keep-order '
            structured_exec "{}"
        ' | jq -s '{
            executed: length,
            parallel: true,
            results: .
        }'
    else
        # Sequential execution
        local results="[]"
        local count=0
        
        while IFS= read -r cmd; do
            local result=$(structured_exec "$cmd")
            results=$(echo "$results" | jq ". + [$result]")
            ((count++))
            
            # Stop on first failure if requested
            local exit_code=$(echo "$result" | jq -r '.exit_code')
            [ "$exit_code" -ne 0 ] && [ "${STOP_ON_ERROR:-false}" = "true" ] && break
        done < <(echo "$commands" | jq -r '.[]')
        
        echo "$results" | jq '{
            executed: '$count',
            parallel: false,
            results: .
        }'
    fi
}

# Pipeline commands with data flow
pipeline_commands() {
    local pipeline="$1"  # JSON array of command objects
    local data=""
    local results="[]"
    
    # Process each stage
    echo "$pipeline" | jq -c '.[]' | while IFS= read -r stage; do
        local cmd=$(echo "$stage" | jq -r '.command')
        local input_type=$(echo "$stage" | jq -r '.input // "stdin"')
        local output_type=$(echo "$stage" | jq -r '.output // "stdout"')
        
        # Prepare command based on input type
        case "$input_type" in
            stdin)
                cmd="echo '$data' | $cmd"
                ;;
            file)
                local temp_file=$(mktemp)
                echo "$data" > "$temp_file"
                cmd="$cmd < $temp_file"
                ;;
            arg)
                cmd="$cmd '$data'"
                ;;
        esac
        
        # Execute stage
        local result=$(structured_exec "$cmd")
        local exit_code=$(echo "$result" | jq -r '.exit_code')
        
        if [ "$exit_code" -eq 0 ]; then
            # Extract output for next stage
            data=$(echo "$result" | jq -r '.stdout')
            results=$(echo "$results" | jq ". + [$result]")
        else
            # Pipeline failed
            echo "$results" | jq ". + [$result]" | jq '{
                pipeline_failed: true,
                failed_at_stage: '$(echo "$results" | jq 'length')',
                results: .
            }'
            return 1
        fi
    done
    
    # Pipeline succeeded
    echo "$results" | jq '{
        pipeline_completed: true,
        stages: length,
        final_output: .[-1].stdout,
        results: .
    }'
}
```

## Usage Examples for Agents

```bash
# Example 1: Safe file operation with validation
safe_read_file "/etc/passwd" 1024

# Example 2: Resource-aware command execution
resource_aware_exec "npm run build" 2147483648 1073741824  # 2GB disk, 1GB RAM

# Example 3: Cached API call
timed_cache "api_users" 300 "curl -s https://api.example.com/users"

# Example 4: Batch operations with error handling
batch_commands '[
    "git status",
    "npm test",
    "docker ps"
]' true 2

# Example 5: Pipeline with data transformation
pipeline_commands '[
    {"command": "cat data.json", "output": "stdout"},
    {"command": "jq .users", "input": "stdin", "output": "stdout"},
    {"command": "wc -l", "input": "stdin"}
]'

# Example 6: Circuit breaker for unreliable service
circuit_breaker "external_api" "curl -s --max-time 5 https://flaky-api.com" 3 30
```

## Agent Best Practices

1. **Always validate inputs** - Never trust external data
2. **Return structured data** - JSON output for parseability
3. **Handle errors gracefully** - No silent failures
4. **Be resource aware** - Check before consuming
5. **Cache when possible** - Reduce redundant operations
6. **Log everything** - Full audit trail
7. **Make operations idempotent** - Safe to retry
8. **Use timeouts** - Prevent hanging
9. **Batch when beneficial** - Reduce overhead
10. **Monitor resource usage** - Stay within limits

Remember: Commands should be predictable, safe, and efficient. Every operation should be traceable and recoverable.
